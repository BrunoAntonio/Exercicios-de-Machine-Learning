{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from ReliefF import ReliefF\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy import stats\n",
    "from scipy import linalg\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import entropy\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.stats.multicomp as multi\n",
    "from scipy.stats import levene\n",
    "from scipy.stats import shapiro\n",
    "from scipy.integrate import cumtrapz\n",
    "from scipy.linalg import eig\n",
    "from numpy.fft import fft\n",
    "from ReliefF import ReliefF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_splitting_TT(X,y,test_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= test_size, random_state=11)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_splitting_TVT(X,y,validation_size,test_size):\n",
    "    X_train1, X_val, y_train1, y_val = train_test_split(X, y, test_size = validation_size, random_state=11)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train1, y_train1, test_size = test_size/(1-validation_size), random_state=11)\n",
    "\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_splitting_K_fold(CV):\n",
    "    kf = KFold(n_splits=CV)\n",
    "    \n",
    "    return kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metricas_exactidao(y_test,predictions):\n",
    "    #Relatório de classificação\n",
    "    #Ver como o modelo se ajusta para os dados de teste\n",
    "    #print(classification_report(y_test, predictions))\n",
    "    report = classification_report(y_test, predictions,output_dict=True)\n",
    "    df=pd.DataFrame(report).transpose()\n",
    "    f1 = df['f1-score'][-1]\n",
    "    precision = df['precision'][-1]\n",
    "    recall = df['recall'][-1]\n",
    "    \n",
    "    confusion = confusion_matrix(y_test,predictions)\n",
    "    #Matriz de confusão\n",
    "    #outside_columns = [\"\",\"Predicted\",\"\"]\n",
    "    #outside_index = [\"\",\"Actual\",\"\"]\n",
    "    #inside = [\"Iris Setosa\",\"Iris Versicolour\",\"Iris Virginica\"]\n",
    "    \n",
    "    #hier_columns = list(zip(outside_columns, inside))\n",
    "    #hier_columns = pd.MultiIndex.from_tuples(hier_columns)\n",
    "    \n",
    "    #hier_index = list(zip(outside_index, inside))\n",
    "    #hier_index = pd.MultiIndex.from_tuples(hier_index)\n",
    "    \n",
    "    \n",
    "    return confusion, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    iris = datasets.load_iris()\n",
    "    df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],columns= iris['feature_names'] + ['target'])\n",
    "    df.head()\n",
    "    \n",
    "    # X - variáveis usadas para modelar o modelo\n",
    "    X = df.drop([\"target\"], axis=1)\n",
    "    # y - variavel que se pretende avaliar\n",
    "    y = df[\"target\"]\n",
    "    \n",
    "    #sns.pairplot(df,hue=\"target\",diag_kind='hist')\n",
    "    #sns.FacetGrid(df,hue=\"target\",size=5).map(sns.distplot,\"petal length (cm)\").add_legend()\n",
    "    #sns.FacetGrid(df,hue=\"target\",size=5).map(sns.distplot,\"petal width (cm)\").add_legend()\n",
    "    #sns.FacetGrid(df,hue=\"target\",size=5).map(sns.distplot,\"sepal length (cm)\").add_legend()\n",
    "    #sns.FacetGrid(df,hue=\"target\",size=5).map(sns.distplot,\"sepal width (cm)\").add_legend()\n",
    "    #plt.show()\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_ReliefF(X,y,numberFeatures,n_neighbors=10):\n",
    "    y_data = y.to_numpy()\n",
    "    features_names = X.columns\n",
    "    X_data = X.to_numpy()\n",
    "    fs = ReliefF(n_neighbors=n_neighbors, n_features_to_keep=numberFeatures)\n",
    "    X_selected = fs.fit_transform(X_data, y_data)\n",
    "    X_selected_name = pd.DataFrame()\n",
    "    for j in range(0,numberFeatures):\n",
    "        found = False\n",
    "        i = 0\n",
    "        while found == False:\n",
    "            compare = (X_selected[:,j] == X_data[:,i])\n",
    "            if sum(compare) == len(X_data[:,i]):\n",
    "                found = True\n",
    "            i += 1\n",
    "        X_selected_name[str(features_names[i-1])] = X_selected[:,j]\n",
    "        \n",
    "    \n",
    "    return X_selected_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_ReliefF_human(X,y,numberFeatures,n_neighbors=10):\n",
    "    y_data = y.to_numpy()\n",
    "    features_names = X.columns\n",
    "    X_data = X.to_numpy()\n",
    "    fs = ReliefF(n_neighbors=n_neighbors, n_features_to_keep=numberFeatures)\n",
    "    X_selected = fs.fit_transform(X_data, y_data)\n",
    "    X_selected_name = pd.DataFrame()\n",
    "    _,selected_features = np.shape(X_selected)\n",
    "    found_column = [0]\n",
    "    for j in range(0,selected_features):\n",
    "        found = False\n",
    "        i = 0\n",
    "        while found == False:\n",
    "            compare = (X_selected[:,j] == X_data[:,i])\n",
    "            if (sum(compare) == len(X_data[:,i])) & (i not in found_column):\n",
    "                found = True\n",
    "                found_column.append(i)\n",
    "            i += 1\n",
    "        X_selected_name[str(features_names[i-1])] = X_selected[:,j]       \n",
    "    \n",
    "    return X_selected_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metric(model, x_test, y_test):\n",
    "    return f1_score(y_test, model.predict(x_test), average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/feature-importance-and-forward-feature-selection-752638849962\n",
    "\n",
    "def forward_feature_selection(X_train, X_test, y_train, y_test,num_features,n_neighbors,distance,weights):\n",
    "    feature_set = []\n",
    "    f1_set = []    \n",
    "    for num_features in range(num_features):\n",
    "        metric_list = [] # Choose appropriate metric based on business problem\n",
    "        model = KNeighborsClassifier(n_neighbors=n_neighbors,metric=distance,weights=weights) # You can choose any model you like, this technique is model agnostic\n",
    "        for feature in X_train.columns:\n",
    "            if feature not in feature_set:\n",
    "                f_set = feature_set.copy()\n",
    "                f_set.append(feature)\n",
    "                model.fit(X_train[f_set], y_train)\n",
    "                metric_list.append((evaluate_metric(model, X_test[f_set], y_test), feature))\n",
    "\n",
    "        metric_list.sort(key=lambda x : x[0], reverse = True) # In case metric follows \"the more, the merrier\"\n",
    "        feature_set.append(metric_list[0][1])\n",
    "        f1_set.append(metric_list[0][0])\n",
    "\n",
    "    #ffs = {feature_set[0]:f1_set[0],feature_set[1]:f1_set[1],feature_set[2]:f1_set[2],feature_set[3]:f1_set[3]}\n",
    "    return feature_set,f1_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex 2.4\n",
    "def dataset_reduced():\n",
    "    X, y = load_dataset()\n",
    "    X[\"target\"] = y\n",
    "    X_0 = X[X[\"target\"]==0]\n",
    "    X_1 = X[X[\"target\"]==1]\n",
    "    X_1 = X_1.sample(30)\n",
    "    X_2 = X[X[\"target\"]==2]\n",
    "    X_2 = X_2.sample(10)\n",
    "    X_reduced = X_0\n",
    "    X_reduced = X_reduced.append(X_1)\n",
    "    X_reduced = X_reduced.append(X_2)\n",
    "    X_reduced = X_reduced.sample(frac = 1)\n",
    "    X_reduced = X_reduced.reset_index(drop=True)\n",
    "    y_reduced = X_reduced[\"target\"]\n",
    "    X_reduced = X_reduced.drop(\"target\",axis=1)\n",
    "    return X_reduced, y_reduced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excercise 3\n",
    "\n",
    "def dataset(patient_id):\n",
    "    head=[\"device_id\",\"acc_x\",\"acc_y\",\"acc_z\",\"gyro_x\",\"gyro_y\",\"gyro_z\",\"mag_x\",\"mag_y\",\"mag_z\",\"time\",\"activity\",\"patient_id\"]\n",
    "\n",
    "    #device_1 = pd.read_csv(\"dataset/part{patient}/part{patient}dev1.csv\".format(patient=patient_id),names=head)\n",
    "    device_2 = pd.read_csv(\"dataset/part{patient}/part{patient}dev2.csv\".format(patient=patient_id),names=head)\n",
    "    #device_3 = pd.read_csv(\"dataset/part{patient}/part{patient}dev3.csv\".format(patient=patient_id),names=head)\n",
    "    #device_4 = pd.read_csv(\"dataset/part{patient}/part{patient}dev4.csv\".format(patient=patient_id),names=head)\n",
    "    #device_5 = pd.read_csv(\"dataset/part{patient}/part{patient}dev5.csv\".format(patient=patient_id),names=head)\n",
    "    \n",
    "    \n",
    "    #df=pd.concat([device_1,device_2,device_3,device_4,device_5],axis=0)\n",
    "    df=device_2\n",
    "    df[\"patient_id\"]=patient_id\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excercise 3\n",
    "\n",
    "def transformed_dataset():\n",
    "\n",
    "    df_transformed = pd.DataFrame()\n",
    "\n",
    "    for i in range (0,15):\n",
    "        df_transformed = df_transformed.append(dataset(i))\n",
    "    df_transformed = df_transformed.reset_index()\n",
    "        \n",
    "    df_transformed[\"acc_vector\"] = df_transformed[\"acc_x\"]**2+df_transformed[\"acc_y\"]**2+df_transformed[\"acc_z\"]**2\n",
    "    df_transformed[\"gyro_vector\"] = df_transformed[\"gyro_x\"]**2+df_transformed[\"gyro_y\"]**2+df_transformed[\"gyro_z\"]**2\n",
    "    df_transformed[\"mag_vector\"] = df_transformed[\"mag_x\"]**2+df_transformed[\"mag_y\"]**2+df_transformed[\"mag_z\"]**2\n",
    "    \n",
    "    return df_transformed[[\"device_id\",\"acc_vector\",\"gyro_vector\",\"mag_vector\",\"time\",\"activity\",\"patient_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice 4.2\n",
    "\n",
    "def dataset_act_vector(act,vector):    \n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    vector_x = vector+'_x'\n",
    "    vector_y = vector+'_y'\n",
    "    vector_z = vector+'_z'\n",
    "\n",
    "    for i in range (0,15):\n",
    "        df = df.append(dataset(i))\n",
    "    df = df.reset_index()\n",
    "    df_act_vector = df[df[\"activity\"]==act][[vector_x,vector_y,vector_z,'time']].reset_index(drop=True)\n",
    "    df_act_vector = df_act_vector.sort_values(by=[\"time\"]).reset_index(drop=True)\n",
    "    \n",
    "    return df_act_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice 4.2\n",
    "\n",
    "def statistical_features(initial,final,act,vector):\n",
    "    \n",
    "    df_act_vector = dataset_act_vector(act,vector)\n",
    "    df_window = df_act_vector[(df_act_vector['time']>=initial) & (df_act_vector['time']<=final)].reset_index(drop=True)\n",
    "    results = df_window.describe()\n",
    "    variance = np.sqrt(results.loc['std']) \n",
    "    results.loc['variance'] = variance\n",
    "    root_mean_square = np.sqrt(np.sum(df_window**2)/df_window.count())\n",
    "    results.loc['rms'] = root_mean_square\n",
    "    vector_x = vector+'_x'\n",
    "    vector_y = vector+'_y'\n",
    "    vector_z = vector+'_z'\n",
    "    derivative = np.zeros((4))\n",
    "    derivative[0] = np.mean(np.gradient(df_window[vector_x],df_window['time']))\n",
    "    derivative[1] = np.mean(np.gradient(df_window[vector_y],df_window['time']))\n",
    "    derivative[2] = np.mean(np.gradient(df_window[vector_z],df_window['time']))\n",
    "    results.loc['av_der'] = derivative\n",
    "    skewness = np.zeros(4)\n",
    "    skewness[0] = skew(df_window[vector_x])\n",
    "    skewness[1] = skew(df_window[vector_y])\n",
    "    skewness[2] = skew(df_window[vector_z])\n",
    "    results.loc['skew'] = skewness\n",
    "    kurt = np.zeros(4)\n",
    "    kurt[0] = kurtosis(df_window[vector_x])\n",
    "    kurt[1] = kurtosis(df_window[vector_y])\n",
    "    kurt[2] = kurtosis(df_window[vector_z])\n",
    "    results.loc['kurtosis'] = kurt\n",
    "    interq_range = results.loc['75%']-results.loc['25%']\n",
    "    results.loc['interq_range'] = interq_range\n",
    "    zero_crossings = np.zeros(4)\n",
    "    #https://stackoverflow.com/questions/3843017/efficiently-detect-sign-changes-in-python\n",
    "    zero_crossings[0] = len(np.where(np.diff(np.sign(df_window[vector_x])))[0])/(final-initial)\n",
    "    zero_crossings[1] = len(np.where(np.diff(np.sign(df_window[vector_y])))[0])/(final-initial)\n",
    "    zero_crossings[2] = len(np.where(np.diff(np.sign(df_window[vector_z])))[0])/(final-initial)\n",
    "    results.loc['zero_crossings'] = zero_crossings\n",
    "    mean_crossings = np.zeros(4)\n",
    "    #https://stackoverflow.com/questions/3843017/efficiently-detect-sign-changes-in-python\n",
    "    mean_crossings[0] = len(np.where(np.diff(np.sign(df_window[vector_x]-results.loc['mean'][vector_x])))[0])/(final-initial)\n",
    "    mean_crossings[1] = len(np.where(np.diff(np.sign(df_window[vector_y]-results.loc['mean'][vector_y])))[0])/(final-initial)\n",
    "    mean_crossings[2] = len(np.where(np.diff(np.sign(df_window[vector_z]-results.loc['mean'][vector_z])))[0])/(final-initial)\n",
    "    results.loc['mean_crossings'] = mean_crossings\n",
    "    pair_correlation = df_window.corr()\n",
    "    results.loc['corr_'+vector_x] = pair_correlation.loc[vector_x]\n",
    "    results.loc['corr_'+vector_y] = pair_correlation.loc[vector_y]\n",
    "    results.loc['corr_'+vector_z] = pair_correlation.loc[vector_z]\n",
    "    spec_entropy = np.zeros(4)\n",
    "    p_data = df_window[vector_x].value_counts()\n",
    "    spec_entropy[0] = entropy(p_data)\n",
    "    p_data = df_window[vector_y].value_counts()\n",
    "    spec_entropy[1] = entropy(p_data)\n",
    "    p_data = df_window[vector_z].value_counts()\n",
    "    spec_entropy[2] = entropy(p_data)\n",
    "    results.loc['spec_entropy'] = spec_entropy\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice 4.2\n",
    "\n",
    "def moviment_intensity(initial,final,act): \n",
    "    vector=\"acc\"\n",
    "    df_act_vector = dataset_act_vector(act,vector)\n",
    "    df_window = df_act_vector[(df_act_vector['time']>=initial) & (df_act_vector['time']<=final)].reset_index(drop=True)\n",
    "    vector_x = \"acc_x\"\n",
    "    vector_y = \"acc_y\"\n",
    "    vector_z = \"acc_z\"\n",
    "    df_window = df_window[[vector_x,vector_y,vector_z]]\n",
    "    MI = np.sqrt(np.sum(df_window**2,axis=1))\n",
    "    AI = np.sum(MI)/(final-initial)\n",
    "    VI = np.sum((MI-AI)**2)/(final-initial)\n",
    "    \n",
    "    return AI,VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice 4.2\n",
    "\n",
    "def normalized_signal_magnitude_area(initial,final,act):\n",
    "    vector=\"acc\"\n",
    "    df_act_vector = dataset_act_vector(act,vector)\n",
    "    df_window = df_act_vector[(df_act_vector['time']>=initial) & (df_act_vector['time']<=final)].reset_index(drop=True)\n",
    "    vector_x = \"acc_x\"\n",
    "    vector_y = \"acc_y\"\n",
    "    vector_z = \"acc_z\"\n",
    "    df_window = df_window[[vector_x,vector_y,vector_z]]\n",
    "    SMA = np.sum(np.sum(abs(df_window)))/(final-initial)\n",
    "    \n",
    "    return SMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice 4.2\n",
    "\n",
    "def eigen_values_dominant_directions(initial,final,act):\n",
    "    vector=\"acc\"\n",
    "    df_act_vector = dataset_act_vector(act,vector)\n",
    "    df_window = df_act_vector[(df_act_vector['time']>=initial) & (df_act_vector['time']<=final)].reset_index(drop=True)\n",
    "    vector_x = \"acc_x\"\n",
    "    vector_y = \"acc_y\"\n",
    "    vector_z = \"acc_z\"\n",
    "    df_window = df_window[[vector_x,vector_y,vector_z]]\n",
    "    covariance_matrix = np.cov(df_window)\n",
    "    eigvals,eigvecs = eig(covariance_matrix)\n",
    "    eigvals = np.array(sorted(eigvals, reverse=True)[:2])\n",
    "    \n",
    "    return eigvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_acceleration_gravity_heading_directions(initial,final,act):\n",
    "    vector=\"acc\"\n",
    "    df_act_vector = dataset_act_vector(act,vector)\n",
    "    df_window = df_act_vector[(df_act_vector['time']>=initial) & (df_act_vector['time']<=final)].reset_index(drop=True)\n",
    "    vector_x = \"acc_x\"\n",
    "    vector_y = \"acc_y\"\n",
    "    vector_z = \"acc_z\"\n",
    "    df_window = df_window[[vector_x,vector_y,vector_z]]\n",
    "    gravity = df_window[vector_x]\n",
    "    heading = df_window[[vector_y,vector_z]]\n",
    "    heading_norm = np.sqrt(np.sum(heading**2,axis=1))\n",
    "    df_gravity_heading = pd.DataFrame({'gravity':gravity,'heading_norm':heading_norm})\n",
    "    correlation = df_gravity_heading.corr()\n",
    "    correlation = correlation[\"heading_norm\"][0]\n",
    "    \n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averaged_velocity_heading_direction(initial,final,act):\n",
    "    vector=\"acc\"\n",
    "    df_act_vector = dataset_act_vector(act,vector)\n",
    "    df_window = df_act_vector[(df_act_vector['time']>=initial) & (df_act_vector['time']<=final)].reset_index(drop=True)\n",
    "    vector_y = vector+'_y'\n",
    "    vector_z = vector+'_z'\n",
    "    velocity_y = np.sum(cumtrapz(df_window[vector_y],df_window['time']))/(final-initial)\n",
    "    velocity_z = np.sum(cumtrapz(df_window[vector_z],df_window['time']))/(final-initial)\n",
    "    norm_velocity = np.sqrt(velocity_y**2 + velocity_z**2)\n",
    "    \n",
    "    return norm_velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice 4.2\n",
    "\n",
    "def averaged_velocity_gravity_direction(initial,final,act):\n",
    "    vector = \"acc\"\n",
    "    df_act_vector = dataset_act_vector(act,vector)\n",
    "    df_window = df_act_vector[(df_act_vector['time']>=initial) & (df_act_vector['time']<=final)].reset_index(drop=True)\n",
    "    vector_x = vector+'_x'\n",
    "    velocity_x = np.sum(cumtrapz(df_window[vector_x],df_window['time']))/(final-initial)\n",
    "    \n",
    "    return velocity_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice 4.2\n",
    "\n",
    "def averaged_rotation_angles_gravity_direction(initial,final,act):\n",
    "    vector=\"gyro\"\n",
    "    df_act_vector = dataset_act_vector(act,vector)\n",
    "    df_window = df_act_vector[(df_act_vector['time']>=initial) & (df_act_vector['time']<=final)].reset_index(drop=True)\n",
    "    vector_x = vector+'_x'\n",
    "    av_rotation = np.sum(df_window[vector_x])/(final-initial)\n",
    "    \n",
    "    return av_rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice 4.2\n",
    "\n",
    "def dominant_frequency(initial,final,act,vector):\n",
    "    df_act_vector = dataset_act_vector(act,vector)\n",
    "    df_window = df_act_vector[(df_act_vector['time']>=initial) & (df_act_vector['time']<=final)].reset_index(drop=True)\n",
    "    vector_x = vector+'_x'\n",
    "    vector_y = vector+'_y'\n",
    "    vector_z = vector+'_z'\n",
    "    df_window = df_window[[vector_x,vector_y,vector_z]]\n",
    "    fourier = fft.fft(df_window)\n",
    "    freq = fft.fftfreq(len(fourier))\n",
    "    index_max = np.unravel_index(abs(fourier).argmax(), fourier.shape)\n",
    "    frequency = freq(index_max[0])\n",
    "    \n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice 4.2\n",
    "\n",
    "def energy(initial,final,act,vector):\n",
    "    df_act_vector = dataset_act_vector(act,vector)\n",
    "    df_window = df_act_vector[(df_act_vector['time']>=initial) & (df_act_vector['time']<=final)].reset_index(drop=True)\n",
    "    vector_x = vector+'_x'\n",
    "    vector_y = vector+'_y'\n",
    "    vector_z = vector+'_z'\n",
    "    df_window = df_window[[vector_x,vector_y,vector_z]]\n",
    "    dc = df_window - df_window.mean()\n",
    "    fourier = np.fft.fft(dc)\n",
    "    en_x,en_y,en_z = abs(fourier).sum(axis=0)/(final-initial)\n",
    "    \n",
    "    return en_x,en_y,en_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice 4.2\n",
    "\n",
    "def averaged_acceleration_energy(initial,final,act):\n",
    "    vector = 'acc'\n",
    "    en_x,en_y,en_z = energy(initial,final,act,vector)\n",
    "    aae = np.mean([en_x,en_y,en_z])\n",
    "    \n",
    "    return aae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice 4.2\n",
    "\n",
    "def averaged_rotation_energy(initial,final,act):\n",
    "    vector = 'gyro'\n",
    "    en_x,en_y,en_z = energy(initial,final,act,vector)\n",
    "    are = np.mean([en_x,en_y,en_z])\n",
    "    \n",
    "    return are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice 4.2\n",
    "\n",
    "#window_size=40000\n",
    "#act=1\n",
    "#vector=\"acc\"\n",
    "\n",
    "def features_dataset_by_activity(window_size,act,vector):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    data = pd.DataFrame(columns=[vector+\"_x_mean\",vector+\"_y_mean\",vector+\"_z_mean\",\n",
    "                                 vector+\"_x_median\",vector+\"_y_median\",vector+\"_z_median\",\n",
    "                                 vector+\"_x_std\",vector+\"_y_std\",vector+\"_z_std\",\n",
    "                                 vector+\"_x_variance\",vector+\"_y_variance\",vector+\"_z_variance\",\n",
    "                                 vector+\"_x_rms\",vector+\"_y_rms\",vector+\"_z_rms\",\n",
    "                               #  vector+\"_x_av_der\",vector+\"_y_av_der\",vector+\"_z_av_der\",\n",
    "                                 vector+\"_x_skew\",vector+\"_y_skew\",vector+\"_z_skew\",\n",
    "                                 vector+\"_x_kurtosis\",vector+\"_y_kurtosis\",vector+\"_z_kurtosis\",\n",
    "                                 vector+\"_x_interq_range\",vector+\"_y_interq_range\",vector+\"_z_interq_range\",\n",
    "                                 vector+\"_x_zero_crossings\",vector+\"_y_zero_crossings\",vector+\"_z_zero_crossings\",\n",
    "                                 vector+\"_x_mean_crossings\",vector+\"_y_mean_crossings\",vector+\"_z_mean_crossings\",\n",
    "                                 vector+\"_x_corr_acc_x\",vector+\"_y_corr_acc_x\",vector+\"_z_corr_acc_x\",\n",
    "                                 vector+\"_x_corr_acc_y\",vector+\"_y_corr_acc_y\",vector+\"_z_corr_acc_y\",\n",
    "                                 vector+\"_x_corr_acc_z\",vector+\"_y_corr_acc_z\",vector+\"_z_corr_acc_z\",\n",
    "                                 vector+\"_x_spec_entropy\",vector+\"_y_spec_entropy\",vector+\"_z_spec_entropy\",\n",
    "                                 \n",
    "                                 \"acc_MI\",\"acc_VI\",\"acc_SMA\",\"acc_gravity_heading_corr\",\"acc_avg_vel_head\", \"acc_avg_vel_grav\", \n",
    "                                 \"gyro_avg_rot\",vector+\"_x_en\",vector+\"_y_en\",vector+\"_z_en\",\"acc_aae\",\"gyro_are\"])\n",
    "\n",
    "    df_win=pd.DataFrame(columns=[\"init\",\"final\"])\n",
    "\n",
    "    df = dataset_act_vector(act,vector)\n",
    "\n",
    "    for i in range(0,len(df)-window_size,int(window_size/2)):\n",
    "        df_win.loc[i]=[df[\"time\"][i],df[\"time\"][i+window_size]]\n",
    "    \n",
    "    df_win=df_win.reset_index(drop=True)\n",
    "    \n",
    "    for i in range(0,len(df_win)):\n",
    "        initial = df_win[\"init\"][i]\n",
    "        final = df_win[\"final\"][i]\n",
    "    \n",
    "        results = statistical_features(initial,final,act,vector)\n",
    "        ai,vi = moviment_intensity(initial,final,act)\n",
    "        sma = normalized_signal_magnitude_area(initial,final,act)\n",
    "        #eigvals = eigen_values_dominant_directions(initial,final,act)\n",
    "        corr = correlation_acceleration_gravity_heading_directions(initial,final,act)\n",
    "        avg_vel_head = averaged_velocity_heading_direction(initial,final,act)\n",
    "        avg_vel_grav = averaged_velocity_gravity_direction(initial,final,act)\n",
    "        avg_rot = averaged_rotation_angles_gravity_direction(initial,final,act)\n",
    "        en_x,en_y,en_z = energy(initial,final,act,vector)\n",
    "        aae = averaged_acceleration_energy(initial,final,act)\n",
    "        are = averaged_rotation_energy(initial,final,act)\n",
    "    \n",
    "        data.loc[i]=[results.loc[\"mean\"][0],results.loc[\"mean\"][1],results.loc[\"mean\"][2],\n",
    "                     results.loc[\"50%\"][0],results.loc[\"50%\"][1],results.loc[\"50%\"][2],\n",
    "                     results.loc[\"std\"][0],results.loc[\"std\"][1],results.loc[\"std\"][2],\n",
    "                     results.loc[\"variance\"][0],results.loc[\"variance\"][1],results.loc[\"variance\"][2],\n",
    "                     results.loc[\"rms\"][0],results.loc[\"rms\"][1],results.loc[\"rms\"][2],\n",
    "                   #  results.loc[\"av_der\"][0],results.loc[\"av_der\"][1],results.loc[\"av_der\"][2],\n",
    "                     results.loc[\"skew\"][0],results.loc[\"skew\"][1],results.loc[\"skew\"][2],\n",
    "                     results.loc[\"kurtosis\"][0],results.loc[\"kurtosis\"][1],results.loc[\"kurtosis\"][2],\n",
    "                     results.loc[\"interq_range\"][0],results.loc[\"interq_range\"][1],results.loc[\"interq_range\"][2],\n",
    "                     results.loc[\"zero_crossings\"][0],results.loc[\"zero_crossings\"][1],results.loc[\"zero_crossings\"][2],\n",
    "                     results.loc[\"mean_crossings\"][0],results.loc[\"mean_crossings\"][1],results.loc[\"mean_crossings\"][2],\n",
    "                     results.loc[\"corr_acc_x\"][0],results.loc[\"corr_acc_x\"][1],results.loc[\"corr_acc_x\"][2],\n",
    "                     results.loc[\"corr_acc_y\"][0],results.loc[\"corr_acc_y\"][1],results.loc[\"corr_acc_y\"][2],\n",
    "                     results.loc[\"corr_acc_z\"][0],results.loc[\"corr_acc_z\"][1],results.loc[\"corr_acc_z\"][2],\n",
    "                     results.loc[\"spec_entropy\"][0],results.loc[\"spec_entropy\"][1],results.loc[\"spec_entropy\"][2],\n",
    "                     \n",
    "                     ai,vi,sma,corr,avg_vel_head,avg_vel_grav,avg_rot,en_x,en_y,en_z,aae,are]\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice 4.2\n",
    "\n",
    "def features_dataset_activities(window_size,vector):\n",
    "\n",
    "    data_final = []\n",
    "    for i in range (1,17):\n",
    "        data = features_dataset_by_activity(window_size,i,vector)\n",
    "        data[\"activity\"] = i\n",
    "        data_final.append(data)\n",
    "    features_data = pd.concat(data_final).reset_index(drop=True)\n",
    "    \n",
    "    return features_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fisher_Score(X_data,y_data):\n",
    "    Score = np.zeros(X_data.shape[1])\n",
    "    categories = np.unique(y_data)\n",
    "    for f in range(0,X_data.shape[1]):\n",
    "        data_feature = X_data.iloc[:,f]\n",
    "        mean_feature = data_feature.mean()\n",
    "        frequency_categories = np.zeros(len(categories))\n",
    "        mean_categories = np.zeros(len(categories))\n",
    "        var_categories = np.zeros(len(categories))\n",
    "        for i in range(0,len(categories)-1):\n",
    "            data_feature_category = data_feature[y_data==categories[i]]\n",
    "            frequency_categories[i] = len(data_feature_category)\n",
    "            mean_categories[i] = data_feature_category.mean()\n",
    "            var_categories[i] = data_feature_category.var()\n",
    "        Score[f] = sum(frequency_categories * ((mean_categories-mean_feature)**2)) / max(sum(frequency_categories * var_categories**2),10**(-5))\n",
    "    return Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_Fisher(features_data,numberFeatures):\n",
    "    y_data = features_data[\"activity\"]\n",
    "    X_data = features_data.drop(columns = \"activity\")\n",
    "    Score = Fisher_Score(X_data,y_data)\n",
    "    Score[np.isnan(Score)] = 0\n",
    "    idx = np.argsort(Score, 0)\n",
    "    idx = idx[::-1]\n",
    "    data_Score = pd.DataFrame()\n",
    "    data_Score = X_data.iloc[:,idx[0:numberFeatures]]\n",
    "    FisherScore_selected = pd.DataFrame([Score[idx[0:numberFeatures]]],columns=data_Score.columns)\n",
    "    \n",
    "    return data_Score, FisherScore_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
